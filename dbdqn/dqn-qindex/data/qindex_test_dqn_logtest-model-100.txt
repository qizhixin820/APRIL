state_dim = 24, action_dim = 72
init_state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  7.8794165   -7.3128967    5.1925235    2.1133108   -5.758173
   2.4330115   -6.9027166   -3.985331     3.6795206   -1.1625456
   6.8021517   -4.319845    -1.9896208  -17.915396    -5.404972
  -0.12774499  -1.8502107  -14.743284     1.4913006    6.960447
  -4.7050524    1.1313497    6.4064198    2.2291636   -1.5621068
   3.37506     -0.85114276   0.15096892   7.592068     1.569168
  -4.1603327   -5.0607486    4.7919474   -2.4210114    8.194166
   7.6081266    2.460137   -15.288371    -2.9650636    2.6684391
   1.9353085   -3.9896736   -9.964772     3.006474     7.290359
   5.1467304    3.5373929   -1.8482296    0.93689656  -3.8034985
   4.100686    -1.4435451    0.80853415  -2.799688     5.9853435
  -6.6605663   -3.4590108   -4.7392797   -3.0014098    8.26626
 -11.76019     -3.921706    -3.7514994   -0.20054328  -3.467383
   5.364052    -2.2108572   -7.4400206    5.6142173   -0.27562717
   4.572311    -4.2797747 ]
action = 59
reward = 0.12683868408203125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  7.222488    -8.155808     4.7298555    2.407493    -5.7542152
   2.8798242   -7.0290456   -4.2862105    2.5232053   -0.7193152
   6.2524633   -3.8304539   -1.1824423  -18.489025    -6.7023306
   0.3109133   -0.63014746 -14.552431     0.9841054    6.5440683
  -4.8215833    1.2398868    7.2367344    3.4007714   -0.8334481
   3.822006    -1.0700451   -0.44166782   6.60442      2.170164
  -3.1932604   -5.094897     5.42268     -1.7180732    8.608933
   7.555804     2.678345   -15.584906    -3.490769     2.2559907
   2.8023741   -3.4775414  -10.399702     2.614805     6.9116974
   4.6062155    2.8310668   -2.630203     1.7223992   -4.5636487
   3.9587226   -2.8254426    1.180486    -3.396797     6.9675717
  -5.908295    -3.0015333   -3.9503064   -3.3230937    7.8910613
 -10.955043    -3.5946333   -3.3015127    0.81385344  -2.8955538
   5.3716044   -1.69485     -6.03444      5.3122826   -0.24579535
   4.2142506   -4.4730926 ]
action = 34
reward = 0.1766681671142578
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.009298324584960938
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.1437664031982422
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.07677078247070312
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.0059604644775390625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.0011920928955078125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.008106231689453125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.03218650817871094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.06556510925292969
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.04887580871582031
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.03743171691894531
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.044345855712890625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.7081031799316406
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.8044242858886719
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.07033348083496094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.17642974853515625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.52642822265625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.15783309936523438
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.7963180541992188
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.1537799835205078
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.23937225341796875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.0045299530029296875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.0007152557373046875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.057220458984375
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.14019012451171875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.11968612670898438
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.1468658447265625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.11205673217773438
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.027894973754882812
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.12183189392089844
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.09226799011230469
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.08678436279296875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.26869773864746094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.07128715515136719
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.24437904357910156
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.00095367431640625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.03528594970703125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.1220703125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.07653236389160156
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.08130073547363281
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.10347366333007812
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.012874603271484375
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.010967254638671875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.07104873657226562
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.04100799560546875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.07653236389160156
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.10251998901367188
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.3829002380371094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.36787986755371094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.1342296600341797
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.19431114196777344
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.1499652862548828
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.30541419982910156
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.022649765014648438
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.0095367431640625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.5371570587158203
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.4603862762451172
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.13136863708496094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.1342296600341797
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.008821487426757812
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.020265579223632812
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.04506111145019531
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.019550323486328125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.05316734313964844
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.005245208740234375
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.06270408630371094
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.014781951904296875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.0059604644775390625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.06914138793945312
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.579833984375
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.514984130859375
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.12493133544921875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.22745132446289062
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.10609626770019531
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.14781951904296875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.08177757263183594
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.05626678466796875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.047206878662109375
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.09131431579589844
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.1327991485595703
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.18668174743652344
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.1990795135498047
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.1163482666015625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.006198883056640625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.13303756713867188
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.225067138671875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.040531158447265625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.044345855712890625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.0629425048828125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.13184547424316406
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.2677440643310547
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.3192424774169922
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.01621246337890625
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.05078315734863281
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.12636184692382812
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = 0.17762184143066406
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.01811981201171875
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.0019073486328125
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
choose action by q_vals: [  8.285468    -8.5098915    4.16186      2.3719804   -5.923931
   2.9760175   -6.4222665   -3.3510702    2.7608354   -1.6803002
   7.7121706   -2.6235108   -0.8350949  -18.357985    -8.156884
   0.04787806  -1.1073446  -13.544478     1.1901665    6.7766805
  -4.404316     1.5674918    6.3500013    3.6954293   -1.1552901
   4.2969046   -0.74208486  -1.7523674    6.4293213    2.0008554
  -3.9559207   -5.2024546    5.291084    -0.24510668   8.552138
   7.4804835    2.6897392  -15.018927    -4.042415     1.6179997
   2.6051698   -4.4643674  -10.5751095    2.665888     6.240844
   4.8764744    2.4477868   -2.9760683    2.870622    -5.5779753
   3.556867    -3.4597423    1.3988093   -3.4181364    6.710725
  -3.7335036   -2.721214    -2.294138    -2.5607011    7.4246397
  -9.413803    -2.4172435   -3.7010896    0.78702134  -1.9304506
   4.321559    -1.8282417   -4.759571     5.33376      0.5838144
   2.5224233   -4.7185555 ]
action = 34
reward = -0.03361701965332031
state = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0]
max reward in all episodes: 0.8044242858886719
max return is 0.36525726318359375at step 73
test using time: 5.851600885391235s
